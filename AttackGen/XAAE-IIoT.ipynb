{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d5591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\NSL-KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b8bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juba\\.conda\\envs\\py3.6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Load the CSV data\n",
    "normal_data = pd.read_csv(\"df_training.csv\")\n",
    "normal_data = normal_data[normal_data['class3'] == 0]\n",
    "normal_data2=normal_data\n",
    "adv_data = pd.read_csv(\"OFF_CWinf-IIoT.csv\")\n",
    "adv_data = adv_data.drop(adv_data.columns[0], axis=1)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the data and convert it to PyTorch tensors\n",
    "normal_data = torch.tensor(normal_data.values[:, :-1], dtype=torch.float32)\n",
    "adv_data = torch.tensor(adv_data.values[:, :-1], dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84275b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Step 4: Train the Autoencoder on normal data\n",
    "input_dim = normal_data.shape[1]\n",
    "latent_dim = 78  # Adjust this\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50 # Adjust this #5BIM;\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = autoencoder(normal_data)\n",
    "    loss = criterion(outputs, normal_data)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Step 5: Extract the latent spaces\n",
    "normal_latent = autoencoder.encoder(normal_data)\n",
    "adv_latent = autoencoder.encoder(adv_data)\n",
    "\n",
    "# Step 6: Create the GAN model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        generated_data = self.model(z)\n",
    "        return generated_data\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        validity = self.model(x)\n",
    "        return validity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f8352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] [Batch 0/79884] Discriminator Loss: 0.6895 Generator Loss: 0.6577\n",
      "[Epoch 1/5] [Batch 4000/79884] Discriminator Loss: 0.5661 Generator Loss: 0.9654\n",
      "[Epoch 1/5] [Batch 8000/79884] Discriminator Loss: 0.4438 Generator Loss: 1.3241\n",
      "[Epoch 1/5] [Batch 12000/79884] Discriminator Loss: 0.5615 Generator Loss: 1.2387\n",
      "[Epoch 1/5] [Batch 16000/79884] Discriminator Loss: 0.4464 Generator Loss: 1.7239\n",
      "[Epoch 1/5] [Batch 20000/79884] Discriminator Loss: 0.6009 Generator Loss: 1.2060\n",
      "[Epoch 1/5] [Batch 24000/79884] Discriminator Loss: 0.5262 Generator Loss: 0.9225\n",
      "[Epoch 1/5] [Batch 28000/79884] Discriminator Loss: 0.3523 Generator Loss: 1.3349\n",
      "[Epoch 1/5] [Batch 32000/79884] Discriminator Loss: 0.4160 Generator Loss: 1.0867\n",
      "[Epoch 1/5] [Batch 36000/79884] Discriminator Loss: 0.2571 Generator Loss: 1.7908\n",
      "[Epoch 1/5] [Batch 40000/79884] Discriminator Loss: 0.6031 Generator Loss: 0.8553\n",
      "[Epoch 1/5] [Batch 44000/79884] Discriminator Loss: 0.4209 Generator Loss: 1.0466\n",
      "[Epoch 1/5] [Batch 48000/79884] Discriminator Loss: 0.6941 Generator Loss: 0.8895\n",
      "[Epoch 1/5] [Batch 52000/79884] Discriminator Loss: 0.6291 Generator Loss: 1.0317\n",
      "[Epoch 1/5] [Batch 56000/79884] Discriminator Loss: 0.6031 Generator Loss: 1.0705\n",
      "[Epoch 1/5] [Batch 60000/79884] Discriminator Loss: 0.4008 Generator Loss: 1.0061\n",
      "[Epoch 1/5] [Batch 64000/79884] Discriminator Loss: 0.4711 Generator Loss: 2.0966\n",
      "[Epoch 1/5] [Batch 68000/79884] Discriminator Loss: 0.4440 Generator Loss: 1.7050\n",
      "[Epoch 1/5] [Batch 72000/79884] Discriminator Loss: 0.4556 Generator Loss: 0.9906\n",
      "[Epoch 1/5] [Batch 76000/79884] Discriminator Loss: 0.4608 Generator Loss: 1.1358\n",
      "[Epoch 2/5] [Batch 0/79884] Discriminator Loss: 0.3327 Generator Loss: 1.8834\n",
      "[Epoch 2/5] [Batch 4000/79884] Discriminator Loss: 0.4399 Generator Loss: 1.2971\n",
      "[Epoch 2/5] [Batch 8000/79884] Discriminator Loss: 0.3816 Generator Loss: 1.4620\n",
      "[Epoch 2/5] [Batch 12000/79884] Discriminator Loss: 0.5026 Generator Loss: 0.9529\n",
      "[Epoch 2/5] [Batch 16000/79884] Discriminator Loss: 0.4796 Generator Loss: 1.0090\n",
      "[Epoch 2/5] [Batch 20000/79884] Discriminator Loss: 0.5085 Generator Loss: 1.6617\n",
      "[Epoch 2/5] [Batch 24000/79884] Discriminator Loss: 0.6724 Generator Loss: 0.9538\n",
      "[Epoch 2/5] [Batch 28000/79884] Discriminator Loss: 0.4521 Generator Loss: 1.4318\n",
      "[Epoch 2/5] [Batch 32000/79884] Discriminator Loss: 0.4881 Generator Loss: 1.2285\n",
      "[Epoch 2/5] [Batch 36000/79884] Discriminator Loss: 0.4175 Generator Loss: 1.1005\n",
      "[Epoch 2/5] [Batch 40000/79884] Discriminator Loss: 0.3100 Generator Loss: 1.6130\n",
      "[Epoch 2/5] [Batch 44000/79884] Discriminator Loss: 0.2371 Generator Loss: 1.6731\n",
      "[Epoch 2/5] [Batch 48000/79884] Discriminator Loss: 0.3813 Generator Loss: 1.6979\n",
      "[Epoch 2/5] [Batch 52000/79884] Discriminator Loss: 0.4184 Generator Loss: 1.9248\n",
      "[Epoch 2/5] [Batch 56000/79884] Discriminator Loss: 0.4592 Generator Loss: 1.0656\n",
      "[Epoch 2/5] [Batch 60000/79884] Discriminator Loss: 0.6193 Generator Loss: 1.6170\n",
      "[Epoch 2/5] [Batch 64000/79884] Discriminator Loss: 0.4239 Generator Loss: 1.1285\n",
      "[Epoch 2/5] [Batch 68000/79884] Discriminator Loss: 0.3047 Generator Loss: 1.3714\n",
      "[Epoch 2/5] [Batch 72000/79884] Discriminator Loss: 0.3456 Generator Loss: 1.4055\n",
      "[Epoch 2/5] [Batch 76000/79884] Discriminator Loss: 0.4037 Generator Loss: 1.3878\n",
      "[Epoch 3/5] [Batch 0/79884] Discriminator Loss: 0.2344 Generator Loss: 1.7511\n",
      "[Epoch 3/5] [Batch 4000/79884] Discriminator Loss: 0.2892 Generator Loss: 2.1573\n",
      "[Epoch 3/5] [Batch 8000/79884] Discriminator Loss: 0.3003 Generator Loss: 1.9984\n",
      "[Epoch 3/5] [Batch 12000/79884] Discriminator Loss: 0.3726 Generator Loss: 1.4670\n",
      "[Epoch 3/5] [Batch 16000/79884] Discriminator Loss: 0.4304 Generator Loss: 1.3637\n",
      "[Epoch 3/5] [Batch 20000/79884] Discriminator Loss: 0.7369 Generator Loss: 1.1741\n",
      "[Epoch 3/5] [Batch 24000/79884] Discriminator Loss: 0.4176 Generator Loss: 1.5011\n",
      "[Epoch 3/5] [Batch 28000/79884] Discriminator Loss: 0.4750 Generator Loss: 1.1459\n",
      "[Epoch 3/5] [Batch 32000/79884] Discriminator Loss: 0.3358 Generator Loss: 1.4072\n",
      "[Epoch 3/5] [Batch 36000/79884] Discriminator Loss: 0.5007 Generator Loss: 1.0988\n",
      "[Epoch 3/5] [Batch 40000/79884] Discriminator Loss: 0.2638 Generator Loss: 2.1236\n",
      "[Epoch 3/5] [Batch 44000/79884] Discriminator Loss: 0.3858 Generator Loss: 1.7369\n",
      "[Epoch 3/5] [Batch 48000/79884] Discriminator Loss: 0.3373 Generator Loss: 1.3270\n",
      "[Epoch 3/5] [Batch 52000/79884] Discriminator Loss: 0.3258 Generator Loss: 2.8396\n",
      "[Epoch 3/5] [Batch 56000/79884] Discriminator Loss: 0.6257 Generator Loss: 1.2895\n",
      "[Epoch 3/5] [Batch 60000/79884] Discriminator Loss: 0.3940 Generator Loss: 1.2264\n",
      "[Epoch 3/5] [Batch 64000/79884] Discriminator Loss: 0.5638 Generator Loss: 1.4906\n",
      "[Epoch 3/5] [Batch 68000/79884] Discriminator Loss: 0.2279 Generator Loss: 1.8382\n",
      "[Epoch 3/5] [Batch 72000/79884] Discriminator Loss: 0.3512 Generator Loss: 1.1966\n",
      "[Epoch 3/5] [Batch 76000/79884] Discriminator Loss: 0.1591 Generator Loss: 2.2552\n",
      "[Epoch 4/5] [Batch 0/79884] Discriminator Loss: 0.3446 Generator Loss: 1.6987\n",
      "[Epoch 4/5] [Batch 4000/79884] Discriminator Loss: 0.2927 Generator Loss: 1.7891\n",
      "[Epoch 4/5] [Batch 8000/79884] Discriminator Loss: 0.3603 Generator Loss: 1.8881\n",
      "[Epoch 4/5] [Batch 12000/79884] Discriminator Loss: 0.3496 Generator Loss: 1.4831\n",
      "[Epoch 4/5] [Batch 16000/79884] Discriminator Loss: 0.3316 Generator Loss: 1.4680\n",
      "[Epoch 4/5] [Batch 20000/79884] Discriminator Loss: 0.3894 Generator Loss: 2.0599\n",
      "[Epoch 4/5] [Batch 24000/79884] Discriminator Loss: 0.3519 Generator Loss: 1.6376\n",
      "[Epoch 4/5] [Batch 28000/79884] Discriminator Loss: 0.3356 Generator Loss: 1.8343\n",
      "[Epoch 4/5] [Batch 32000/79884] Discriminator Loss: 0.5228 Generator Loss: 1.5811\n",
      "[Epoch 4/5] [Batch 36000/79884] Discriminator Loss: 0.2804 Generator Loss: 1.6489\n",
      "[Epoch 4/5] [Batch 40000/79884] Discriminator Loss: 0.4350 Generator Loss: 1.8189\n",
      "[Epoch 4/5] [Batch 44000/79884] Discriminator Loss: 0.2517 Generator Loss: 2.1772\n",
      "[Epoch 4/5] [Batch 48000/79884] Discriminator Loss: 0.4236 Generator Loss: 1.4969\n",
      "[Epoch 4/5] [Batch 52000/79884] Discriminator Loss: 0.2625 Generator Loss: 1.6728\n",
      "[Epoch 4/5] [Batch 56000/79884] Discriminator Loss: 0.3156 Generator Loss: 2.0412\n",
      "[Epoch 4/5] [Batch 60000/79884] Discriminator Loss: 0.0960 Generator Loss: 2.6151\n",
      "[Epoch 4/5] [Batch 64000/79884] Discriminator Loss: 0.9034 Generator Loss: 1.1620\n",
      "[Epoch 4/5] [Batch 68000/79884] Discriminator Loss: 0.8456 Generator Loss: 1.2972\n",
      "[Epoch 4/5] [Batch 72000/79884] Discriminator Loss: 0.3467 Generator Loss: 1.3068\n",
      "[Epoch 4/5] [Batch 76000/79884] Discriminator Loss: 0.2137 Generator Loss: 2.1977\n",
      "[Epoch 5/5] [Batch 0/79884] Discriminator Loss: 0.2731 Generator Loss: 1.5360\n",
      "[Epoch 5/5] [Batch 4000/79884] Discriminator Loss: 0.1949 Generator Loss: 1.7951\n",
      "[Epoch 5/5] [Batch 8000/79884] Discriminator Loss: 0.3949 Generator Loss: 1.0785\n",
      "[Epoch 5/5] [Batch 12000/79884] Discriminator Loss: 0.3445 Generator Loss: 1.4841\n",
      "[Epoch 5/5] [Batch 16000/79884] Discriminator Loss: 0.3806 Generator Loss: 1.6074\n",
      "[Epoch 5/5] [Batch 20000/79884] Discriminator Loss: 0.8333 Generator Loss: 1.4231\n",
      "[Epoch 5/5] [Batch 24000/79884] Discriminator Loss: 0.4233 Generator Loss: 1.5041\n",
      "[Epoch 5/5] [Batch 28000/79884] Discriminator Loss: 0.2259 Generator Loss: 1.8591\n",
      "[Epoch 5/5] [Batch 32000/79884] Discriminator Loss: 0.2534 Generator Loss: 1.9497\n",
      "[Epoch 5/5] [Batch 36000/79884] Discriminator Loss: 0.4763 Generator Loss: 1.5143\n",
      "[Epoch 5/5] [Batch 40000/79884] Discriminator Loss: 0.3535 Generator Loss: 1.8260\n",
      "[Epoch 5/5] [Batch 44000/79884] Discriminator Loss: 0.2189 Generator Loss: 2.0719\n",
      "[Epoch 5/5] [Batch 48000/79884] Discriminator Loss: 0.2680 Generator Loss: 2.2847\n",
      "[Epoch 5/5] [Batch 52000/79884] Discriminator Loss: 1.6267 Generator Loss: 2.4514\n",
      "[Epoch 5/5] [Batch 56000/79884] Discriminator Loss: 0.3957 Generator Loss: 3.0334\n",
      "[Epoch 5/5] [Batch 60000/79884] Discriminator Loss: 0.2558 Generator Loss: 2.1968\n",
      "[Epoch 5/5] [Batch 64000/79884] Discriminator Loss: 0.4218 Generator Loss: 1.4362\n",
      "[Epoch 5/5] [Batch 68000/79884] Discriminator Loss: 0.2044 Generator Loss: 2.1206\n",
      "[Epoch 5/5] [Batch 72000/79884] Discriminator Loss: 0.4451 Generator Loss: 1.4571\n",
      "[Epoch 5/5] [Batch 76000/79884] Discriminator Loss: 0.1719 Generator Loss: 2.1465\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 78 # Adjust this\n",
    "output_dim = input_dim  # Adjust this\n",
    "generator = Generator(latent_dim, output_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "batch_size=32\n",
    "# Step 7: Train the GAN model\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "num_epochs =5#0 Adjust this #5:FSFM; BIM;\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(adv_data), batch_size):\n",
    "        real_data = normal_data[i:i+batch_size].to(device)\n",
    "        adv_samples = adv_data[i:i+batch_size].to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((real_data.size(0), 1)).to(device)\n",
    "        fake = torch.zeros((real_data.size(0), 1)).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Generate a batch of fake samples from adversarial samples\n",
    "        fake_data = generator(adv_samples)\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_data), valid[:real_data.size(0)])\n",
    "        fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake[:fake_data.size(0)])\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of fake samples from adversarial samples\n",
    "        fake_data = generator(adv_samples)\n",
    "\n",
    "        # Measure generator's ability to fool the discriminator\n",
    "        generator_loss = adversarial_loss(discriminator(fake_data), valid[:fake_data.size(0)])\n",
    "\n",
    "        generator_loss.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i}/{len(adv_data)}] \"\n",
    "                f\"Discriminator Loss: {discriminator_loss.item():.4f} \"\n",
    "                f\"Generator Loss: {generator_loss.item():.4f}\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7731b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Generate new samples with the GAN\n",
    "num_samples = len(adv_data)\n",
    "z = torch.randn((num_samples, latent_dim)).to(device)\n",
    "generated_samples = generator(z).detach().cpu()\n",
    "file_Name='CWinf-XAAE-IIoT.csv'\n",
    "# Step 9: Decode the generated samples using the Autoencoder\n",
    "decoded_samples = autoencoder.decoder(generated_samples).detach().cpu()\n",
    "\n",
    "# Step 10: Save the decoded samples as CSV file\n",
    "output_data = pd.DataFrame(decoded_samples.numpy(), columns=[f\"Feature_{i}\" for i in range(decoded_samples.shape[1])])\n",
    "\n",
    "# Set the label column based on the length of adv_data\n",
    "output_data['class3'] = np.ones(len(output_data))\n",
    "output_data.columns = normal_data2.columns\n",
    "output_data.to_csv(file_Name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c221ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ff06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#file_Name='BIM-XAAE-KDD.csv'\n",
    "# Load the datasets\n",
    "data_x1 = pd.read_csv('clean_examples.csv')  # Original dataset\n",
    "data_x2 = pd.read_csv(file_Name)  # Perturbed dataset\n",
    "# List of columns to keep in x2\n",
    "columns_to_keep =['Scr_IP','Scr_port', 'Des_IP', 'Des_port','Servicewebsocket', 'Servicessh', 'Servicesmtp', 'Servicesimple_service_discovery', 'Serviceprivate', 'Serviceother', 'Servicenetbios-ns', 'Servicemysql', 'Servicemqtt', 'Servicemodbus', 'Serviceimap', 'Servicehttps', 'Servicehttp', 'Serviceecho', 'Servicedns', 'Servicedhcp', 'Servicecoap', 'Protocoludp', 'Protocoltcp', 'Protocolicmp', 'Conn_state', 'is_syn_only', 'Is_SYN_ACK', 'is_pure_ack', 'is_with_payload', 'FIN or RST', 'OSSEC_alert', 'Login_attempt', 'Succesful_login', 'File_activity', 'Process_activity', 'read_write_physical.process', 'is_privileged']\n",
    "\n",
    "\n",
    "# Ensure column names are consistent and exist in both dataframes\n",
    "columns_to_keep = [col for col in columns_to_keep if col in data_x2.columns and col in data_x1.columns]\n",
    "\n",
    "# Columns to replace in x2 with values from x1 (all columns not in 'columns_to_keep')\n",
    "columns_to_replace = [col for col in data_x2.columns if col not in columns_to_keep]\n",
    "\n",
    "# Replace the values\n",
    "data_x2[columns_to_replace] = data_x1[columns_to_replace]\n",
    "# Save the modified x2 dataset\n",
    "data_x2.to_csv(file_Name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664f4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positive rows (no negative values): 79884\n",
      "Total number of negative rows (at least one negative value): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_Name)\n",
    "\n",
    "# Initialize counters for positive and negative rows\n",
    "positive_rows = 0\n",
    "negative_rows = 0\n",
    "\n",
    "# Iterate through each row and check for negative values\n",
    "for index, row in df.iterrows():\n",
    "    if (row < 0).any():  # If any value in the row is negative\n",
    "        negative_rows += 1\n",
    "    else:  # If there are no negative values in the row\n",
    "        positive_rows += 1\n",
    "\n",
    "# Print the total counts\n",
    "print(f\"Total number of positive rows (no negative values): {positive_rows}\")\n",
    "print(f\"Total number of negative rows (at least one negative value): {negative_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5f17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
