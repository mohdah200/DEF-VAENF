{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d5591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\NSL-KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b8bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juba\\.conda\\envs\\py3.6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU usage before training:  33.8\n",
      "Memory usage before training:  271851520\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "import shap\n",
    "print('CPU usage before training: ', psutil.cpu_percent())  # in percentage\n",
    "# Train the model\n",
    "# Monitor the memory usage\n",
    "process = psutil.Process()\n",
    "print('Memory usage before training: ', process.memory_info().rss)  # in bytes\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Load the CSV data\n",
    "normal_data = pd.read_csv(\"df_training-tmp.csv\")\n",
    "normal_data = normal_data[normal_data['class3'] == 0]\n",
    "normal_data2=normal_data\n",
    "adv_data = pd.read_csv(\"OFF_JSMA.csv\")\n",
    "adv_data = adv_data.drop(adv_data.columns[0], axis=1)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the data and convert it to PyTorch tensors\n",
    "normal_data = torch.tensor(normal_data.values[:, :-1], dtype=torch.float32)\n",
    "adv_data = torch.tensor(adv_data.values[:, :-1], dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84275b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Step 4: Train the Autoencoder on normal data\n",
    "input_dim = normal_data.shape[1]\n",
    "latent_dim = 78  # Adjust this\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50 # Adjust this #5BIM;\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = autoencoder(normal_data)\n",
    "    loss = criterion(outputs, normal_data)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Step 5: Extract the latent spaces\n",
    "normal_latent = autoencoder.encoder(normal_data)\n",
    "adv_latent = autoencoder.encoder(adv_data)\n",
    "\n",
    "# Step 6: Create the GAN model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        generated_data = self.model(z)\n",
    "        return generated_data\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        validity = self.model(x)\n",
    "        return validity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f8352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] [Batch 0/79884] Discriminator Loss: 0.6953 Generator Loss: 0.7223\n",
      "[Epoch 1/5] [Batch 4000/79884] Discriminator Loss: 0.7438 Generator Loss: 0.9434\n",
      "[Epoch 1/5] [Batch 8000/79884] Discriminator Loss: 0.6515 Generator Loss: 0.8488\n",
      "[Epoch 1/5] [Batch 12000/79884] Discriminator Loss: 0.4497 Generator Loss: 0.9382\n",
      "[Epoch 1/5] [Batch 16000/79884] Discriminator Loss: 0.5469 Generator Loss: 0.8984\n",
      "[Epoch 1/5] [Batch 20000/79884] Discriminator Loss: 0.5791 Generator Loss: 0.8213\n",
      "[Epoch 1/5] [Batch 24000/79884] Discriminator Loss: 0.7654 Generator Loss: 0.5963\n",
      "[Epoch 1/5] [Batch 28000/79884] Discriminator Loss: 0.4790 Generator Loss: 1.3840\n",
      "[Epoch 1/5] [Batch 32000/79884] Discriminator Loss: 0.4107 Generator Loss: 0.9602\n",
      "[Epoch 1/5] [Batch 36000/79884] Discriminator Loss: 0.5480 Generator Loss: 0.8699\n",
      "[Epoch 1/5] [Batch 40000/79884] Discriminator Loss: 0.4473 Generator Loss: 1.0347\n",
      "[Epoch 1/5] [Batch 44000/79884] Discriminator Loss: 0.5473 Generator Loss: 1.1954\n",
      "[Epoch 1/5] [Batch 48000/79884] Discriminator Loss: 0.6419 Generator Loss: 1.2050\n",
      "[Epoch 1/5] [Batch 52000/79884] Discriminator Loss: 0.5318 Generator Loss: 1.0601\n",
      "[Epoch 1/5] [Batch 56000/79884] Discriminator Loss: 0.6695 Generator Loss: 0.9166\n",
      "[Epoch 1/5] [Batch 60000/79884] Discriminator Loss: 0.4724 Generator Loss: 0.9191\n",
      "[Epoch 1/5] [Batch 64000/79884] Discriminator Loss: 0.4487 Generator Loss: 1.0299\n",
      "[Epoch 1/5] [Batch 68000/79884] Discriminator Loss: 0.4681 Generator Loss: 1.3064\n",
      "[Epoch 1/5] [Batch 72000/79884] Discriminator Loss: 0.3974 Generator Loss: 1.3956\n",
      "[Epoch 1/5] [Batch 76000/79884] Discriminator Loss: 0.5676 Generator Loss: 1.9047\n",
      "[Epoch 2/5] [Batch 0/79884] Discriminator Loss: 0.5229 Generator Loss: 1.3939\n",
      "[Epoch 2/5] [Batch 4000/79884] Discriminator Loss: 0.4056 Generator Loss: 1.1388\n",
      "[Epoch 2/5] [Batch 8000/79884] Discriminator Loss: 0.4920 Generator Loss: 0.9236\n",
      "[Epoch 2/5] [Batch 12000/79884] Discriminator Loss: 0.4121 Generator Loss: 1.1755\n",
      "[Epoch 2/5] [Batch 16000/79884] Discriminator Loss: 0.2689 Generator Loss: 1.8685\n",
      "[Epoch 2/5] [Batch 20000/79884] Discriminator Loss: 0.4430 Generator Loss: 1.1025\n",
      "[Epoch 2/5] [Batch 24000/79884] Discriminator Loss: 0.2218 Generator Loss: 2.5635\n",
      "[Epoch 2/5] [Batch 28000/79884] Discriminator Loss: 0.3813 Generator Loss: 1.1810\n",
      "[Epoch 2/5] [Batch 32000/79884] Discriminator Loss: 0.6043 Generator Loss: 1.3559\n",
      "[Epoch 2/5] [Batch 36000/79884] Discriminator Loss: 0.6126 Generator Loss: 0.8248\n",
      "[Epoch 2/5] [Batch 40000/79884] Discriminator Loss: 0.7282 Generator Loss: 0.9152\n",
      "[Epoch 2/5] [Batch 44000/79884] Discriminator Loss: 0.5772 Generator Loss: 0.9020\n",
      "[Epoch 2/5] [Batch 48000/79884] Discriminator Loss: 0.5977 Generator Loss: 0.9631\n",
      "[Epoch 2/5] [Batch 52000/79884] Discriminator Loss: 0.5666 Generator Loss: 0.8523\n",
      "[Epoch 2/5] [Batch 56000/79884] Discriminator Loss: 0.5080 Generator Loss: 1.0273\n",
      "[Epoch 2/5] [Batch 60000/79884] Discriminator Loss: 0.5176 Generator Loss: 0.9912\n",
      "[Epoch 2/5] [Batch 64000/79884] Discriminator Loss: 0.6033 Generator Loss: 1.0168\n",
      "[Epoch 2/5] [Batch 68000/79884] Discriminator Loss: 0.5177 Generator Loss: 1.0080\n",
      "[Epoch 2/5] [Batch 72000/79884] Discriminator Loss: 0.6875 Generator Loss: 1.3736\n",
      "[Epoch 2/5] [Batch 76000/79884] Discriminator Loss: 0.7498 Generator Loss: 0.8577\n",
      "[Epoch 3/5] [Batch 0/79884] Discriminator Loss: 0.5469 Generator Loss: 1.2439\n",
      "[Epoch 3/5] [Batch 4000/79884] Discriminator Loss: 0.5972 Generator Loss: 0.8595\n",
      "[Epoch 3/5] [Batch 8000/79884] Discriminator Loss: 0.6835 Generator Loss: 0.8108\n",
      "[Epoch 3/5] [Batch 12000/79884] Discriminator Loss: 0.5071 Generator Loss: 1.1247\n",
      "[Epoch 3/5] [Batch 16000/79884] Discriminator Loss: 0.5820 Generator Loss: 1.1692\n",
      "[Epoch 3/5] [Batch 20000/79884] Discriminator Loss: 0.5896 Generator Loss: 0.9581\n",
      "[Epoch 3/5] [Batch 24000/79884] Discriminator Loss: 0.4160 Generator Loss: 1.5543\n",
      "[Epoch 3/5] [Batch 28000/79884] Discriminator Loss: 0.4228 Generator Loss: 1.1559\n",
      "[Epoch 3/5] [Batch 32000/79884] Discriminator Loss: 0.4739 Generator Loss: 1.1283\n",
      "[Epoch 3/5] [Batch 36000/79884] Discriminator Loss: 0.5760 Generator Loss: 1.1794\n",
      "[Epoch 3/5] [Batch 40000/79884] Discriminator Loss: 0.8301 Generator Loss: 1.1451\n",
      "[Epoch 3/5] [Batch 44000/79884] Discriminator Loss: 0.6763 Generator Loss: 0.8984\n",
      "[Epoch 3/5] [Batch 48000/79884] Discriminator Loss: 0.5290 Generator Loss: 1.0757\n",
      "[Epoch 3/5] [Batch 52000/79884] Discriminator Loss: 0.8897 Generator Loss: 0.8571\n",
      "[Epoch 3/5] [Batch 56000/79884] Discriminator Loss: 0.5853 Generator Loss: 0.8348\n",
      "[Epoch 3/5] [Batch 60000/79884] Discriminator Loss: 0.6064 Generator Loss: 1.0523\n",
      "[Epoch 3/5] [Batch 64000/79884] Discriminator Loss: 0.6101 Generator Loss: 1.0205\n",
      "[Epoch 3/5] [Batch 68000/79884] Discriminator Loss: 0.4646 Generator Loss: 1.1207\n",
      "[Epoch 3/5] [Batch 72000/79884] Discriminator Loss: 0.3408 Generator Loss: 1.3974\n",
      "[Epoch 3/5] [Batch 76000/79884] Discriminator Loss: 0.6786 Generator Loss: 1.1901\n",
      "[Epoch 4/5] [Batch 0/79884] Discriminator Loss: 0.4174 Generator Loss: 1.6991\n",
      "[Epoch 4/5] [Batch 4000/79884] Discriminator Loss: 0.5277 Generator Loss: 0.9884\n",
      "[Epoch 4/5] [Batch 8000/79884] Discriminator Loss: 0.7301 Generator Loss: 0.9113\n",
      "[Epoch 4/5] [Batch 12000/79884] Discriminator Loss: 0.5035 Generator Loss: 1.0751\n",
      "[Epoch 4/5] [Batch 16000/79884] Discriminator Loss: 0.2853 Generator Loss: 1.6600\n",
      "[Epoch 4/5] [Batch 20000/79884] Discriminator Loss: 0.5931 Generator Loss: 0.9712\n",
      "[Epoch 4/5] [Batch 24000/79884] Discriminator Loss: 0.4060 Generator Loss: 1.2462\n",
      "[Epoch 4/5] [Batch 28000/79884] Discriminator Loss: 0.4987 Generator Loss: 1.1244\n",
      "[Epoch 4/5] [Batch 32000/79884] Discriminator Loss: 0.4013 Generator Loss: 1.1783\n",
      "[Epoch 4/5] [Batch 36000/79884] Discriminator Loss: 0.4287 Generator Loss: 1.4467\n",
      "[Epoch 4/5] [Batch 40000/79884] Discriminator Loss: 0.6682 Generator Loss: 1.1298\n",
      "[Epoch 4/5] [Batch 44000/79884] Discriminator Loss: 0.4362 Generator Loss: 1.1596\n",
      "[Epoch 4/5] [Batch 48000/79884] Discriminator Loss: 0.4754 Generator Loss: 1.4890\n",
      "[Epoch 4/5] [Batch 52000/79884] Discriminator Loss: 0.3985 Generator Loss: 1.1872\n",
      "[Epoch 4/5] [Batch 56000/79884] Discriminator Loss: 0.5144 Generator Loss: 1.1384\n",
      "[Epoch 4/5] [Batch 60000/79884] Discriminator Loss: 0.3754 Generator Loss: 1.3139\n",
      "[Epoch 4/5] [Batch 64000/79884] Discriminator Loss: 0.0963 Generator Loss: 3.0586\n",
      "[Epoch 4/5] [Batch 68000/79884] Discriminator Loss: 0.6980 Generator Loss: 1.4048\n",
      "[Epoch 4/5] [Batch 72000/79884] Discriminator Loss: 0.5918 Generator Loss: 1.1640\n",
      "[Epoch 4/5] [Batch 76000/79884] Discriminator Loss: 0.4554 Generator Loss: 1.1515\n",
      "[Epoch 5/5] [Batch 0/79884] Discriminator Loss: 0.5809 Generator Loss: 1.1041\n",
      "[Epoch 5/5] [Batch 4000/79884] Discriminator Loss: 0.5015 Generator Loss: 1.3241\n",
      "[Epoch 5/5] [Batch 8000/79884] Discriminator Loss: 0.4060 Generator Loss: 1.6820\n",
      "[Epoch 5/5] [Batch 12000/79884] Discriminator Loss: 0.3222 Generator Loss: 2.2926\n",
      "[Epoch 5/5] [Batch 16000/79884] Discriminator Loss: 0.3396 Generator Loss: 1.2685\n",
      "[Epoch 5/5] [Batch 20000/79884] Discriminator Loss: 0.4999 Generator Loss: 1.2646\n",
      "[Epoch 5/5] [Batch 24000/79884] Discriminator Loss: 0.5809 Generator Loss: 1.3489\n",
      "[Epoch 5/5] [Batch 28000/79884] Discriminator Loss: 0.4338 Generator Loss: 1.2254\n",
      "[Epoch 5/5] [Batch 32000/79884] Discriminator Loss: 0.3863 Generator Loss: 1.1686\n",
      "[Epoch 5/5] [Batch 36000/79884] Discriminator Loss: 0.3776 Generator Loss: 1.3716\n",
      "[Epoch 5/5] [Batch 40000/79884] Discriminator Loss: 0.3979 Generator Loss: 1.2642\n",
      "[Epoch 5/5] [Batch 44000/79884] Discriminator Loss: 0.6140 Generator Loss: 1.3717\n",
      "[Epoch 5/5] [Batch 48000/79884] Discriminator Loss: 0.4233 Generator Loss: 1.0748\n",
      "[Epoch 5/5] [Batch 52000/79884] Discriminator Loss: 0.3804 Generator Loss: 1.4400\n",
      "[Epoch 5/5] [Batch 56000/79884] Discriminator Loss: 0.4153 Generator Loss: 1.2608\n",
      "[Epoch 5/5] [Batch 60000/79884] Discriminator Loss: 0.5415 Generator Loss: 1.3126\n",
      "[Epoch 5/5] [Batch 64000/79884] Discriminator Loss: 0.4129 Generator Loss: 1.1076\n",
      "[Epoch 5/5] [Batch 68000/79884] Discriminator Loss: 0.4561 Generator Loss: 1.5332\n",
      "[Epoch 5/5] [Batch 72000/79884] Discriminator Loss: 0.6880 Generator Loss: 1.1119\n",
      "[Epoch 5/5] [Batch 76000/79884] Discriminator Loss: 0.4969 Generator Loss: 1.0329\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 78 # Adjust this\n",
    "output_dim = input_dim  # Adjust this\n",
    "generator = Generator(latent_dim, output_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "batch_size=32\n",
    "# Step 7: Train the GAN model\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "num_epochs =5#0 Adjust this #5:FSFM; BIM;\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(adv_data), batch_size):\n",
    "        real_data = normal_data[i:i+batch_size].to(device)\n",
    "        adv_samples = adv_data[i:i+batch_size].to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((real_data.size(0), 1)).to(device)\n",
    "        fake = torch.zeros((real_data.size(0), 1)).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Generate a batch of fake samples from adversarial samples\n",
    "        fake_data = generator(adv_samples)\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_data), valid[:real_data.size(0)])\n",
    "        fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake[:fake_data.size(0)])\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of fake samples from adversarial samples\n",
    "        fake_data = generator(adv_samples)\n",
    "\n",
    "        # Measure generator's ability to fool the discriminator\n",
    "        generator_loss = adversarial_loss(discriminator(fake_data), valid[:fake_data.size(0)])\n",
    "\n",
    "        generator_loss.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i}/{len(adv_data)}] \"\n",
    "                f\"Discriminator Loss: {discriminator_loss.item():.4f} \"\n",
    "                f\"Generator Loss: {generator_loss.item():.4f}\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7731b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Generate new samples with the GAN\n",
    "num_samples = len(adv_data)\n",
    "z = torch.randn((num_samples, latent_dim)).to(device)\n",
    "generated_samples = generator(z).detach().cpu()\n",
    "file_Name='JSMA-XAAE-IIoT-tmp.csv'\n",
    "# Step 9: Decode the generated samples using the Autoencoder\n",
    "decoded_samples = autoencoder.decoder(generated_samples).detach().cpu()\n",
    "\n",
    "# Step 10: Save the decoded samples as CSV file\n",
    "output_data = pd.DataFrame(decoded_samples.numpy(), columns=[f\"Feature_{i}\" for i in range(decoded_samples.shape[1])])\n",
    "\n",
    "# Set the label column based on the length of adv_data\n",
    "output_data['class3'] = np.ones(len(output_data))\n",
    "output_data.columns = normal_data2.columns\n",
    "output_data.to_csv(file_Name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c221ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ff06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#file_Name='BIM-XAAE-KDD.csv'\n",
    "# Load the datasets\n",
    "data_x1 = pd.read_csv('clean_examples.csv')  # Original dataset\n",
    "data_x2 = pd.read_csv(file_Name)  # Perturbed dataset\n",
    "# List of columns to keep in x2\n",
    "columns_to_keep =['Scr_IP','Scr_port', 'Des_IP', 'Des_port','Servicewebsocket', 'Servicessh', 'Servicesmtp', 'Servicesimple_service_discovery', 'Serviceprivate', 'Serviceother', 'Servicenetbios-ns', 'Servicemysql', 'Servicemqtt', 'Servicemodbus', 'Serviceimap', 'Servicehttps', 'Servicehttp', 'Serviceecho', 'Servicedns', 'Servicedhcp', 'Servicecoap', 'Protocoludp', 'Protocoltcp', 'Protocolicmp', 'Conn_state', 'is_syn_only', 'Is_SYN_ACK', 'is_pure_ack', 'is_with_payload', 'FIN or RST', 'OSSEC_alert', 'Login_attempt', 'Succesful_login', 'File_activity', 'Process_activity', 'read_write_physical.process', 'is_privileged']\n",
    "\n",
    "\n",
    "# Ensure column names are consistent and exist in both dataframes\n",
    "columns_to_keep = [col for col in columns_to_keep if col in data_x2.columns and col in data_x1.columns]\n",
    "\n",
    "# Columns to replace in x2 with values from x1 (all columns not in 'columns_to_keep')\n",
    "columns_to_replace = [col for col in data_x2.columns if col not in columns_to_keep]\n",
    "\n",
    "# Replace the values\n",
    "data_x2[columns_to_replace] = data_x1[columns_to_replace]\n",
    "# Save the modified x2 dataset\n",
    "data_x2.to_csv(file_Name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c08510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after training:  1043976192\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "CPU usage after training:  49.6\n",
      "Training time:  173.38813638687134\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "end_time = time.time()\n",
    "print('Memory usage after training: ', process.memory_info().rss)  # in bytes\n",
    "print(\"A\"*50)\n",
    "print('CPU usage after training: ', psutil.cpu_percent())  # in percentage\n",
    "print('Training time: ', end_time - start_time)  # in se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664f4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positive rows (no negative values): 79884\n",
      "Total number of negative rows (at least one negative value): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_Name)\n",
    "\n",
    "# Initialize counters for positive and negative rows\n",
    "positive_rows = 0\n",
    "negative_rows = 0\n",
    "\n",
    "# Iterate through each row and check for negative values\n",
    "for index, row in df.iterrows():\n",
    "    if (row < 0).any():  # If any value in the row is negative\n",
    "        negative_rows += 1\n",
    "    else:  # If there are no negative values in the row\n",
    "        positive_rows += 1\n",
    "\n",
    "# Print the total counts\n",
    "print(f\"Total number of positive rows (no negative values): {positive_rows}\")\n",
    "print(f\"Total number of negative rows (at least one negative value): {negative_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5f17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
