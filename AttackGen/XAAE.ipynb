{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129fcbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juba\\.conda\\envs\\py3.6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Load the CSV data\n",
    "normal_data = pd.read_csv(\"normal.csv\")\n",
    "adv_data = pd.read_csv(\"adjusted_adversarial_examples_CWinf.csv\")\n",
    "adv_data = adv_data.drop(adv_data.columns[0], axis=1)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the data and convert it to PyTorch tensors\n",
    "normal_data = torch.tensor(normal_data.values[:, :-1], dtype=torch.float32)\n",
    "adv_data = torch.tensor(adv_data.values[:, :-1], dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1005099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Step 4: Train the Autoencoder on normal data\n",
    "input_dim = normal_data.shape[1]\n",
    "latent_dim = 100  # Adjust this\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50 # Adjust this #5BIM;\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = autoencoder(normal_data)\n",
    "    loss = criterion(outputs, normal_data)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Step 5: Extract the latent spaces\n",
    "normal_latent = autoencoder.encoder(normal_data)\n",
    "adv_latent = autoencoder.encoder(adv_data)\n",
    "\n",
    "# Step 6: Create the GAN model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        generated_data = self.model(z)\n",
    "        return generated_data\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        validity = self.model(x)\n",
    "        return validity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f8352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] [Batch 0/109128] Discriminator Loss: 0.4416 Generator Loss: 0.6937\n",
      "[Epoch 1/5] [Batch 4000/109128] Discriminator Loss: 0.3426 Generator Loss: 0.7227\n",
      "[Epoch 1/5] [Batch 8000/109128] Discriminator Loss: 0.2546 Generator Loss: 0.9683\n",
      "[Epoch 1/5] [Batch 12000/109128] Discriminator Loss: 0.2765 Generator Loss: 0.9663\n",
      "[Epoch 1/5] [Batch 16000/109128] Discriminator Loss: 0.4169 Generator Loss: 0.7641\n",
      "[Epoch 1/5] [Batch 20000/109128] Discriminator Loss: 0.4500 Generator Loss: 0.5272\n",
      "[Epoch 1/5] [Batch 24000/109128] Discriminator Loss: 0.2292 Generator Loss: 1.2992\n",
      "[Epoch 1/5] [Batch 28000/109128] Discriminator Loss: 0.0735 Generator Loss: 2.4040\n",
      "[Epoch 1/5] [Batch 32000/109128] Discriminator Loss: 0.4064 Generator Loss: 0.7442\n",
      "[Epoch 1/5] [Batch 36000/109128] Discriminator Loss: 0.1053 Generator Loss: 1.8298\n",
      "[Epoch 1/5] [Batch 40000/109128] Discriminator Loss: 4.8083 Generator Loss: 1.8239\n",
      "[Epoch 1/5] [Batch 44000/109128] Discriminator Loss: 0.1072 Generator Loss: 2.2301\n",
      "[Epoch 1/5] [Batch 48000/109128] Discriminator Loss: 0.0882 Generator Loss: 2.1310\n",
      "[Epoch 1/5] [Batch 52000/109128] Discriminator Loss: 0.4916 Generator Loss: 0.7001\n",
      "[Epoch 1/5] [Batch 56000/109128] Discriminator Loss: 0.5752 Generator Loss: 0.6355\n",
      "[Epoch 1/5] [Batch 60000/109128] Discriminator Loss: 0.8096 Generator Loss: 1.4338\n",
      "[Epoch 1/5] [Batch 64000/109128] Discriminator Loss: 1.3514 Generator Loss: 2.3913\n",
      "[Epoch 1/5] [Batch 68000/109128] Discriminator Loss: 0.5185 Generator Loss: 0.5331\n",
      "[Epoch 1/5] [Batch 72000/109128] Discriminator Loss: 0.2607 Generator Loss: 0.9063\n",
      "[Epoch 1/5] [Batch 76000/109128] Discriminator Loss: 0.4185 Generator Loss: 0.7458\n",
      "[Epoch 1/5] [Batch 80000/109128] Discriminator Loss: 1.6523 Generator Loss: 2.0337\n",
      "[Epoch 1/5] [Batch 84000/109128] Discriminator Loss: 0.5277 Generator Loss: 0.9288\n",
      "[Epoch 1/5] [Batch 88000/109128] Discriminator Loss: 0.1869 Generator Loss: 1.2975\n",
      "[Epoch 1/5] [Batch 92000/109128] Discriminator Loss: 1.1965 Generator Loss: 2.5620\n",
      "[Epoch 1/5] [Batch 96000/109128] Discriminator Loss: 0.0466 Generator Loss: 3.0222\n",
      "[Epoch 1/5] [Batch 100000/109128] Discriminator Loss: 0.7255 Generator Loss: 0.6356\n",
      "[Epoch 1/5] [Batch 104000/109128] Discriminator Loss: 0.0551 Generator Loss: 3.6909\n",
      "[Epoch 1/5] [Batch 108000/109128] Discriminator Loss: 9.3828 Generator Loss: 5.1312\n",
      "[Epoch 2/5] [Batch 0/109128] Discriminator Loss: 1.5891 Generator Loss: 4.3263\n",
      "[Epoch 2/5] [Batch 4000/109128] Discriminator Loss: 6.5944 Generator Loss: 2.2718\n",
      "[Epoch 2/5] [Batch 8000/109128] Discriminator Loss: 0.0924 Generator Loss: 4.3702\n",
      "[Epoch 2/5] [Batch 12000/109128] Discriminator Loss: 1.6177 Generator Loss: 2.8435\n",
      "[Epoch 2/5] [Batch 16000/109128] Discriminator Loss: 1.7731 Generator Loss: 3.5284\n",
      "[Epoch 2/5] [Batch 20000/109128] Discriminator Loss: 0.0181 Generator Loss: 3.7158\n",
      "[Epoch 2/5] [Batch 24000/109128] Discriminator Loss: 0.0486 Generator Loss: 3.0089\n",
      "[Epoch 2/5] [Batch 28000/109128] Discriminator Loss: 0.0636 Generator Loss: 3.0733\n",
      "[Epoch 2/5] [Batch 32000/109128] Discriminator Loss: 0.0129 Generator Loss: 3.3801\n",
      "[Epoch 2/5] [Batch 36000/109128] Discriminator Loss: 0.0895 Generator Loss: 3.0806\n",
      "[Epoch 2/5] [Batch 40000/109128] Discriminator Loss: 0.0241 Generator Loss: 4.1945\n",
      "[Epoch 2/5] [Batch 44000/109128] Discriminator Loss: 0.0012 Generator Loss: 6.2321\n",
      "[Epoch 2/5] [Batch 48000/109128] Discriminator Loss: 0.0035 Generator Loss: 5.7691\n",
      "[Epoch 2/5] [Batch 52000/109128] Discriminator Loss: 0.0103 Generator Loss: 4.8863\n",
      "[Epoch 2/5] [Batch 56000/109128] Discriminator Loss: 0.0006 Generator Loss: 7.2548\n",
      "[Epoch 2/5] [Batch 60000/109128] Discriminator Loss: 0.0016 Generator Loss: 5.8710\n",
      "[Epoch 2/5] [Batch 64000/109128] Discriminator Loss: 0.6828 Generator Loss: 1.1847\n",
      "[Epoch 2/5] [Batch 68000/109128] Discriminator Loss: 3.1356 Generator Loss: 4.0699\n",
      "[Epoch 2/5] [Batch 72000/109128] Discriminator Loss: 0.6546 Generator Loss: 1.3891\n",
      "[Epoch 2/5] [Batch 76000/109128] Discriminator Loss: 0.3732 Generator Loss: 0.9494\n",
      "[Epoch 2/5] [Batch 80000/109128] Discriminator Loss: 0.0002 Generator Loss: 8.9128\n",
      "[Epoch 2/5] [Batch 84000/109128] Discriminator Loss: 0.3593 Generator Loss: 0.8648\n",
      "[Epoch 2/5] [Batch 88000/109128] Discriminator Loss: 0.8299 Generator Loss: 1.1133\n",
      "[Epoch 2/5] [Batch 92000/109128] Discriminator Loss: 0.0472 Generator Loss: 2.4051\n",
      "[Epoch 2/5] [Batch 96000/109128] Discriminator Loss: 1.5706 Generator Loss: 4.3886\n",
      "[Epoch 2/5] [Batch 100000/109128] Discriminator Loss: 0.0094 Generator Loss: 4.5095\n",
      "[Epoch 2/5] [Batch 104000/109128] Discriminator Loss: 1.5706 Generator Loss: 4.9504\n",
      "[Epoch 2/5] [Batch 108000/109128] Discriminator Loss: 0.0184 Generator Loss: 4.3219\n",
      "[Epoch 3/5] [Batch 0/109128] Discriminator Loss: 0.0278 Generator Loss: 4.0619\n",
      "[Epoch 3/5] [Batch 4000/109128] Discriminator Loss: 0.5330 Generator Loss: 1.8808\n",
      "[Epoch 3/5] [Batch 8000/109128] Discriminator Loss: 1.6034 Generator Loss: 3.7260\n",
      "[Epoch 3/5] [Batch 12000/109128] Discriminator Loss: 3.1342 Generator Loss: 4.3603\n",
      "[Epoch 3/5] [Batch 16000/109128] Discriminator Loss: 1.5904 Generator Loss: 4.7743\n",
      "[Epoch 3/5] [Batch 20000/109128] Discriminator Loss: 1.5625 Generator Loss: 11.2446\n",
      "[Epoch 3/5] [Batch 24000/109128] Discriminator Loss: 0.4124 Generator Loss: 1.6306\n",
      "[Epoch 3/5] [Batch 28000/109128] Discriminator Loss: 2.0967 Generator Loss: 1.9016\n",
      "[Epoch 3/5] [Batch 32000/109128] Discriminator Loss: 14.0679 Generator Loss: 4.7225\n",
      "[Epoch 3/5] [Batch 36000/109128] Discriminator Loss: 6.5232 Generator Loss: 2.5188\n",
      "[Epoch 3/5] [Batch 40000/109128] Discriminator Loss: 6.2507 Generator Loss: 9.6888\n",
      "[Epoch 3/5] [Batch 44000/109128] Discriminator Loss: 17.4634 Generator Loss: 13.9263\n",
      "[Epoch 3/5] [Batch 48000/109128] Discriminator Loss: 14.0671 Generator Loss: 8.1303\n",
      "[Epoch 3/5] [Batch 52000/109128] Discriminator Loss: 25.0250 Generator Loss: 14.5240\n",
      "[Epoch 3/5] [Batch 56000/109128] Discriminator Loss: 1.5708 Generator Loss: 6.0354\n",
      "[Epoch 3/5] [Batch 60000/109128] Discriminator Loss: 0.0048 Generator Loss: 8.3113\n",
      "[Epoch 3/5] [Batch 64000/109128] Discriminator Loss: 1.5763 Generator Loss: 5.1393\n",
      "[Epoch 3/5] [Batch 68000/109128] Discriminator Loss: 0.0267 Generator Loss: 5.1947\n",
      "[Epoch 3/5] [Batch 72000/109128] Discriminator Loss: 0.0028 Generator Loss: 9.7217\n",
      "[Epoch 3/5] [Batch 76000/109128] Discriminator Loss: 1.5740 Generator Loss: 6.0771\n",
      "[Epoch 3/5] [Batch 80000/109128] Discriminator Loss: 4.7052 Generator Loss: 5.9739\n",
      "[Epoch 3/5] [Batch 84000/109128] Discriminator Loss: 4.6875 Generator Loss: 22.4581\n",
      "[Epoch 3/5] [Batch 88000/109128] Discriminator Loss: 2.4232 Generator Loss: 0.2120\n",
      "[Epoch 3/5] [Batch 92000/109128] Discriminator Loss: 0.0000 Generator Loss: 10.3465\n",
      "[Epoch 3/5] [Batch 96000/109128] Discriminator Loss: 0.0000 Generator Loss: 12.5001\n",
      "[Epoch 3/5] [Batch 100000/109128] Discriminator Loss: 9.3787 Generator Loss: 8.1129\n",
      "[Epoch 3/5] [Batch 104000/109128] Discriminator Loss: 12.5311 Generator Loss: 5.8262\n",
      "[Epoch 3/5] [Batch 108000/109128] Discriminator Loss: 15.6408 Generator Loss: 6.8399\n",
      "[Epoch 4/5] [Batch 0/109128] Discriminator Loss: 8.6198 Generator Loss: 10.7695\n",
      "[Epoch 4/5] [Batch 4000/109128] Discriminator Loss: 17.2134 Generator Loss: 6.8351\n",
      "[Epoch 4/5] [Batch 8000/109128] Discriminator Loss: 15.6250 Generator Loss: 20.1552\n",
      "[Epoch 4/5] [Batch 12000/109128] Discriminator Loss: 0.0000 Generator Loss: 11.4957\n",
      "[Epoch 4/5] [Batch 16000/109128] Discriminator Loss: 0.0360 Generator Loss: 7.3968\n",
      "[Epoch 4/5] [Batch 20000/109128] Discriminator Loss: 0.0008 Generator Loss: 8.2015\n",
      "[Epoch 4/5] [Batch 24000/109128] Discriminator Loss: 0.0148 Generator Loss: 7.9542\n",
      "[Epoch 4/5] [Batch 28000/109128] Discriminator Loss: 1.5725 Generator Loss: 7.4574\n",
      "[Epoch 4/5] [Batch 32000/109128] Discriminator Loss: 6.2506 Generator Loss: 7.9047\n",
      "[Epoch 4/5] [Batch 36000/109128] Discriminator Loss: 0.0303 Generator Loss: 6.9777\n",
      "[Epoch 4/5] [Batch 40000/109128] Discriminator Loss: 0.0000 Generator Loss: 11.9530\n",
      "[Epoch 4/5] [Batch 44000/109128] Discriminator Loss: 4.2414 Generator Loss: 13.6668\n",
      "[Epoch 4/5] [Batch 48000/109128] Discriminator Loss: 0.0020 Generator Loss: 7.6320\n",
      "[Epoch 4/5] [Batch 52000/109128] Discriminator Loss: 0.0000 Generator Loss: 12.0801\n",
      "[Epoch 4/5] [Batch 56000/109128] Discriminator Loss: 0.0000 Generator Loss: 14.9586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/5] [Batch 60000/109128] Discriminator Loss: 0.0000 Generator Loss: 13.9437\n",
      "[Epoch 4/5] [Batch 64000/109128] Discriminator Loss: 4.6901 Generator Loss: 7.2283\n",
      "[Epoch 4/5] [Batch 68000/109128] Discriminator Loss: 0.0296 Generator Loss: 8.3184\n",
      "[Epoch 4/5] [Batch 72000/109128] Discriminator Loss: 0.0057 Generator Loss: 10.0365\n",
      "[Epoch 4/5] [Batch 76000/109128] Discriminator Loss: 1.5625 Generator Loss: 13.2009\n",
      "[Epoch 4/5] [Batch 80000/109128] Discriminator Loss: 9.6243 Generator Loss: 5.4003\n",
      "[Epoch 4/5] [Batch 84000/109128] Discriminator Loss: 1.5625 Generator Loss: 22.3098\n",
      "[Epoch 4/5] [Batch 88000/109128] Discriminator Loss: 3.1250 Generator Loss: 45.5087\n",
      "[Epoch 4/5] [Batch 92000/109128] Discriminator Loss: 0.0000 Generator Loss: 18.1833\n",
      "[Epoch 4/5] [Batch 96000/109128] Discriminator Loss: 4.6875 Generator Loss: 17.8668\n",
      "[Epoch 4/5] [Batch 100000/109128] Discriminator Loss: 3.4832 Generator Loss: 6.7911\n",
      "[Epoch 4/5] [Batch 104000/109128] Discriminator Loss: 0.0379 Generator Loss: 6.9533\n",
      "[Epoch 4/5] [Batch 108000/109128] Discriminator Loss: 0.0000 Generator Loss: 17.0185\n",
      "[Epoch 5/5] [Batch 0/109128] Discriminator Loss: 0.0001 Generator Loss: 11.3224\n",
      "[Epoch 5/5] [Batch 4000/109128] Discriminator Loss: 3.1265 Generator Loss: 10.1634\n",
      "[Epoch 5/5] [Batch 8000/109128] Discriminator Loss: 0.0000 Generator Loss: 55.1071\n",
      "[Epoch 5/5] [Batch 12000/109128] Discriminator Loss: 1.5634 Generator Loss: 11.9230\n",
      "[Epoch 5/5] [Batch 16000/109128] Discriminator Loss: 10.9375 Generator Loss: 40.3940\n",
      "[Epoch 5/5] [Batch 20000/109128] Discriminator Loss: 9.4321 Generator Loss: 12.7791\n",
      "[Epoch 5/5] [Batch 24000/109128] Discriminator Loss: 9.3750 Generator Loss: 11.8788\n",
      "[Epoch 5/5] [Batch 28000/109128] Discriminator Loss: 0.0014 Generator Loss: 13.3498\n",
      "[Epoch 5/5] [Batch 32000/109128] Discriminator Loss: 17.1881 Generator Loss: 12.4867\n",
      "[Epoch 5/5] [Batch 36000/109128] Discriminator Loss: 9.3827 Generator Loss: 11.2747\n",
      "[Epoch 5/5] [Batch 40000/109128] Discriminator Loss: 6.2500 Generator Loss: 18.8670\n",
      "[Epoch 5/5] [Batch 44000/109128] Discriminator Loss: 3.2459 Generator Loss: 7.9950\n",
      "[Epoch 5/5] [Batch 48000/109128] Discriminator Loss: 8.3999 Generator Loss: 4.5541\n",
      "[Epoch 5/5] [Batch 52000/109128] Discriminator Loss: 3.1252 Generator Loss: 9.9453\n",
      "[Epoch 5/5] [Batch 56000/109128] Discriminator Loss: 6.2556 Generator Loss: 9.3225\n",
      "[Epoch 5/5] [Batch 60000/109128] Discriminator Loss: 1.5682 Generator Loss: 8.9138\n",
      "[Epoch 5/5] [Batch 64000/109128] Discriminator Loss: 5.4550 Generator Loss: 9.7279\n",
      "[Epoch 5/5] [Batch 68000/109128] Discriminator Loss: 4.6878 Generator Loss: 12.3138\n",
      "[Epoch 5/5] [Batch 72000/109128] Discriminator Loss: 10.9375 Generator Loss: 21.7756\n",
      "[Epoch 5/5] [Batch 76000/109128] Discriminator Loss: 7.9931 Generator Loss: 10.4839\n",
      "[Epoch 5/5] [Batch 80000/109128] Discriminator Loss: 1.5797 Generator Loss: 9.7523\n",
      "[Epoch 5/5] [Batch 84000/109128] Discriminator Loss: 9.3750 Generator Loss: 19.2149\n",
      "[Epoch 5/5] [Batch 88000/109128] Discriminator Loss: 17.7555 Generator Loss: 16.3993\n",
      "[Epoch 5/5] [Batch 92000/109128] Discriminator Loss: 10.9375 Generator Loss: 32.1048\n",
      "[Epoch 5/5] [Batch 96000/109128] Discriminator Loss: 23.4389 Generator Loss: 13.2090\n",
      "[Epoch 5/5] [Batch 100000/109128] Discriminator Loss: 26.5627 Generator Loss: 11.7189\n",
      "[Epoch 5/5] [Batch 104000/109128] Discriminator Loss: 17.1879 Generator Loss: 10.8527\n",
      "[Epoch 5/5] [Batch 108000/109128] Discriminator Loss: 26.5626 Generator Loss: 10.7214\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100 # Adjust this\n",
    "output_dim = input_dim  # Adjust this\n",
    "generator = Generator(latent_dim, output_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "batch_size=32\n",
    "# Step 7: Train the GAN model\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "num_epochs =5#0 Adjust this #5:FSFM; BIM;\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(adv_data), batch_size):\n",
    "        real_data = normal_data[i:i+batch_size].to(device)\n",
    "        adv_samples = adv_data[i:i+batch_size].to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((real_data.size(0), 1)).to(device)\n",
    "        fake = torch.zeros((real_data.size(0), 1)).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Generate a batch of fake samples from adversarial samples\n",
    "        fake_data = generator(adv_samples)\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_data), valid[:real_data.size(0)])\n",
    "        fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake[:fake_data.size(0)])\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of fake samples from adversarial samples\n",
    "        fake_data = generator(adv_samples)\n",
    "\n",
    "        # Measure generator's ability to fool the discriminator\n",
    "        generator_loss = adversarial_loss(discriminator(fake_data), valid[:fake_data.size(0)])\n",
    "\n",
    "        generator_loss.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i}/{len(adv_data)}] \"\n",
    "                f\"Discriminator Loss: {discriminator_loss.item():.4f} \"\n",
    "                f\"Generator Loss: {generator_loss.item():.4f}\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7731b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Generate new samples with the GAN\n",
    "num_samples = len(adv_data)\n",
    "z = torch.randn((num_samples, latent_dim)).to(device)\n",
    "generated_samples = generator(z).detach().cpu()\n",
    "\n",
    "# Step 9: Decode the generated samples using the Autoencoder\n",
    "decoded_samples = autoencoder.decoder(generated_samples).detach().cpu()\n",
    "\n",
    "# Step 10: Save the decoded samples as CSV file\n",
    "output_data = pd.DataFrame(decoded_samples.numpy(), columns=[f\"Feature_{i}\" for i in range(decoded_samples.shape[1])])\n",
    "\n",
    "# Set the label column based on the length of adv_data\n",
    "output_data['Label'] = np.ones(len(output_data))\n",
    "\n",
    "output_data.to_csv(\"modified_adv_samplesADvNormalCWinf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff06cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
