{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ee0694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juba\\.conda\\envs\\py3.6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "lb_make=LabelEncoder()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f534d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\XAAE-IIoT\n"
     ]
    }
   ],
   "source": [
    "cd C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\XAAE-IIoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4063218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "#cd C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\NSL-KDD\n",
    "missing_values = [\"n/a\", \"na\", \"Infinity\", \"NaN\",\"nan\",\"-\",\"excel\",\"?\",\"#DIV/0!\",\"aza\"]\n",
    "df = pd.read_csv(\"df_training.csv\", na_values = missing_values, engine='python', skipinitialspace=True)\n",
    "df=df.fillna(df.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8ad6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(656666, 79)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4db388",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = df[df['class3'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290b4f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scr_IP</th>\n",
       "      <th>Servicewebsocket</th>\n",
       "      <th>Servicessh</th>\n",
       "      <th>Servicesmtp</th>\n",
       "      <th>Servicesimple_service_discovery</th>\n",
       "      <th>Serviceprivate</th>\n",
       "      <th>Serviceother</th>\n",
       "      <th>Servicenetbios-ns</th>\n",
       "      <th>Servicemysql</th>\n",
       "      <th>Servicemqtt</th>\n",
       "      <th>...</th>\n",
       "      <th>Avg_num_cswch/s</th>\n",
       "      <th>std_num_cswch/s</th>\n",
       "      <th>OSSEC_alert</th>\n",
       "      <th>OSSEC_alert_level</th>\n",
       "      <th>Login_attempt</th>\n",
       "      <th>Succesful_login</th>\n",
       "      <th>File_activity</th>\n",
       "      <th>Process_activity</th>\n",
       "      <th>read_write_physical.process</th>\n",
       "      <th>is_privileged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319533</th>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114983</td>\n",
       "      <td>0.254258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319534</th>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114439</td>\n",
       "      <td>0.263953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319535</th>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092399</td>\n",
       "      <td>0.223940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319536</th>\n",
       "      <td>0.085714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118655</td>\n",
       "      <td>0.300815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319537</th>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101665</td>\n",
       "      <td>0.245578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656661</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104036</td>\n",
       "      <td>0.238433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656662</th>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101960</td>\n",
       "      <td>0.240807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656663</th>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103872</td>\n",
       "      <td>0.254394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656664</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104247</td>\n",
       "      <td>0.245080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656665</th>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102792</td>\n",
       "      <td>0.233497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337133 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Scr_IP  Servicewebsocket  Servicessh  Servicesmtp  \\\n",
       "319533  0.542857               0.0         0.0          0.0   \n",
       "319534  0.542857               0.0         0.0          0.0   \n",
       "319535  0.457143               0.0         0.0          0.0   \n",
       "319536  0.085714               1.0         0.0          0.0   \n",
       "319537  0.314286               0.0         0.0          0.0   \n",
       "...          ...               ...         ...          ...   \n",
       "656661  0.571429               0.0         0.0          0.0   \n",
       "656662  0.457143               0.0         0.0          0.0   \n",
       "656663  0.542857               0.0         0.0          0.0   \n",
       "656664  0.571429               0.0         0.0          0.0   \n",
       "656665  0.457143               0.0         0.0          0.0   \n",
       "\n",
       "        Servicesimple_service_discovery  Serviceprivate  Serviceother  \\\n",
       "319533                              0.0             0.0           0.0   \n",
       "319534                              0.0             0.0           0.0   \n",
       "319535                              0.0             0.0           0.0   \n",
       "319536                              0.0             0.0           0.0   \n",
       "319537                              0.0             0.0           0.0   \n",
       "...                                 ...             ...           ...   \n",
       "656661                              0.0             0.0           0.0   \n",
       "656662                              0.0             0.0           0.0   \n",
       "656663                              0.0             0.0           0.0   \n",
       "656664                              0.0             0.0           0.0   \n",
       "656665                              0.0             0.0           0.0   \n",
       "\n",
       "        Servicenetbios-ns  Servicemysql  Servicemqtt  ...  Avg_num_cswch/s  \\\n",
       "319533                0.0           0.0          0.0  ...         0.114983   \n",
       "319534                0.0           0.0          0.0  ...         0.114439   \n",
       "319535                0.0           0.0          0.0  ...         0.092399   \n",
       "319536                0.0           0.0          0.0  ...         0.118655   \n",
       "319537                0.0           0.0          0.0  ...         0.101665   \n",
       "...                   ...           ...          ...  ...              ...   \n",
       "656661                0.0           0.0          0.0  ...         0.104036   \n",
       "656662                0.0           0.0          0.0  ...         0.101960   \n",
       "656663                0.0           0.0          0.0  ...         0.103872   \n",
       "656664                0.0           0.0          0.0  ...         0.104247   \n",
       "656665                0.0           0.0          0.0  ...         0.102792   \n",
       "\n",
       "        std_num_cswch/s  OSSEC_alert  OSSEC_alert_level  Login_attempt  \\\n",
       "319533         0.254258          0.0                0.0            1.0   \n",
       "319534         0.263953          0.0                0.0            0.0   \n",
       "319535         0.223940          0.0                0.0            0.0   \n",
       "319536         0.300815          0.0                0.0            0.0   \n",
       "319537         0.245578          0.0                0.0            0.0   \n",
       "...                 ...          ...                ...            ...   \n",
       "656661         0.238433          0.0                0.0            0.0   \n",
       "656662         0.240807          0.0                0.0            0.0   \n",
       "656663         0.254394          0.0                0.0            0.0   \n",
       "656664         0.245080          0.0                0.0            0.0   \n",
       "656665         0.233497          0.0                0.0            0.0   \n",
       "\n",
       "        Succesful_login  File_activity  Process_activity  \\\n",
       "319533              1.0            1.0               1.0   \n",
       "319534              0.0            0.0               0.0   \n",
       "319535              0.0            0.0               0.0   \n",
       "319536              0.0            0.0               0.0   \n",
       "319537              0.0            0.0               0.0   \n",
       "...                 ...            ...               ...   \n",
       "656661              0.0            0.0               0.0   \n",
       "656662              0.0            0.0               0.0   \n",
       "656663              0.0            0.0               0.0   \n",
       "656664              0.0            0.0               0.0   \n",
       "656665              0.0            0.0               0.0   \n",
       "\n",
       "        read_write_physical.process  is_privileged  \n",
       "319533                          1.0            1.0  \n",
       "319534                          1.0            0.0  \n",
       "319535                          0.0            0.0  \n",
       "319536                          1.0            0.0  \n",
       "319537                          0.0            0.0  \n",
       "...                             ...            ...  \n",
       "656661                          1.0            0.0  \n",
       "656662                          0.0            0.0  \n",
       "656663                          1.0            0.0  \n",
       "656664                          1.0            0.0  \n",
       "656665                          0.0            0.0  \n",
       "\n",
       "[337133 rows x 78 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d852828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Normalize data\\nscaler = StandardScaler()\\nnormal_data_scaled = scaler.fit_transform(normal_data)\\n\\n# Convert to PyTorch tensors\\n#X_train, X_test = train_test_split(normal_data_scaled, test_size=0.2, random_state=42)\\nX_train = torch.tensor(normal_data_scaled, dtype=torch.float)\\n\\n# DataLoader\\ntrain_loader = DataLoader(TensorDataset(X_train), batch_size=32, shuffle=True)\\n#test_loader = DataLoader(TensorDataset(X_test), batch_size=32)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "normal_data_scaled = scaler.fit_transform(normal_data)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "#X_train, X_test = train_test_split(normal_data_scaled, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(normal_data_scaled, dtype=torch.float)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train), batch_size=32, shuffle=True)\n",
    "#test_loader = DataLoader(TensorDataset(X_test), batch_size=32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89ca1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" class VAE(nn.Module):\\n    def __init__(self, input_dim, latent_dim):\\n        super(VAE, self).__init__()\\n        self.fc1 = nn.Linear(input_dim, 512)\\n        self.fc21 = nn.Linear(512, latent_dim)  # Mean of the latent space\\n        self.fc22 = nn.Linear(512, latent_dim)  # Standard deviation of the latent space\\n        self.fc3 = nn.Linear(latent_dim, 512)\\n        self.fc4 = nn.Linear(512, input_dim)\\n\\n    def encode(self, x):\\n        h1 = torch.relu(self.fc1(x))\\n        return self.fc21(h1), self.fc22(h1)\\n\\n    def reparameterize(self, mu, logvar):\\n        std = torch.exp(0.5*logvar)\\n        eps = torch.randn_like(std)\\n        return mu + eps*std\\n\\n    def decode(self, z):\\n        h3 = torch.relu(self.fc3(z))\\n        return torch.sigmoid(self.fc4(h3))\\n\\n    def forward(self, x):\\n        mu, logvar = self.encode(x.view(-1, 78))\\n        z = self.reparameterize(mu, logvar)\\n        return self.decode(z), mu, logvar\\n        # Loss Function\\ndef loss_function(recon_x, x, mu, logvar):\\n    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 78), reduction='sum')\\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n    return BCE + KLD\\n\\n# Model, Optimizer\\nmodel = VAE(input_dim=78, latent_dim=20)\\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc21 = nn.Linear(512, latent_dim)  # Mean of the latent space\n",
    "        self.fc22 = nn.Linear(512, latent_dim)  # Standard deviation of the latent space\n",
    "        self.fc3 = nn.Linear(latent_dim, 512)\n",
    "        self.fc4 = nn.Linear(512, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 78))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "        # Loss Function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 78), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Model, Optimizer\n",
    "model = VAE(input_dim=78, latent_dim=20)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1700d2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337133, 78)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c33dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "#df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Filter out normal data and drop the class column\n",
    "#normal_data = df[df['class3'] == 0].drop('class3', axis=1)\n",
    "\n",
    "# Normalize data\n",
    "\n",
    "\n",
    "X = normal_data.drop('class3', axis=1).values\n",
    "y = normal_data['class3'].values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_tensor, torch.tensor(y, dtype=torch.float32)), batch_size=32, shuffle=True)\n",
    "\n",
    "# DataLoader\n",
    "#train_loader = DataLoader(TensorDataset(X), batch_size=32, shuffle=True)\n",
    "#test_loader = DataLoader(TensorDataset(X_test), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a806a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class FusionVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(FusionVAE, self).__init__()\n",
    "        self.denoising_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.denoising_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.disentangled_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.disentangled_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        denoising_latent = self.denoising_encoder(x)\n",
    "        disentangled_latent = self.disentangled_encoder(x)\n",
    "        mu = self.fc1(denoising_latent)\n",
    "        logvar = self.fc2(denoising_latent)\n",
    "        return mu, logvar, disentangled_latent\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar, disentangled_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        combined_latent = z + disentangled_latent\n",
    "        denoising_decoded = self.denoising_decoder(z)\n",
    "        disentangled_decoded = self.disentangled_decoder(combined_latent)\n",
    "        decoded = (denoising_decoded + disentangled_decoded) / 2\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "# Loss Function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 78), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9e649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for data, _ in data_loader:  # Ignore labels\n",
    "            reconstruction, _, _ = model(data)\n",
    "            batch_errors = torch.mean((data - reconstruction) ** 2, dim=1)  # Mean squared error per example\n",
    "            reconstruction_errors.extend(batch_errors.tolist())\n",
    "    \n",
    "    # Determine the 95th percentile as the threshold\n",
    "    threshold = np.percentile(reconstruction_errors, 95)\n",
    "    return threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5beb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d309772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Anomaly Detection Threshold: 0.017692868877202273\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Training Loop Placeholder\n",
    "def train(model, train_loader, num_epochs=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Calculate and Save Thresholds\n",
    "thresholds = []\n",
    "# In the cross-validation loop, when you define train_loader and val_loader\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_tensor)):\n",
    "    # Assuming you're working with unsupervised data and don't actually have labels\n",
    "    # Create dummy labels just to satisfy DataLoader requirements\n",
    "    dummy_labels_train = torch.zeros(len(train_idx))\n",
    "    dummy_labels_val = torch.zeros(len(val_idx))\n",
    "    \n",
    "    # Create the DataLoaders with tuples of (data, dummy_labels)\n",
    "    train_loader = DataLoader(TensorDataset(X_tensor[train_idx], dummy_labels_train), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_tensor[val_idx], dummy_labels_val), batch_size=32)\n",
    "\n",
    "    # Continue as before\n",
    "\n",
    "    train(model, train_loader)\n",
    "    threshold = calculate_threshold(model, val_loader)\n",
    "    thresholds.append(threshold)\n",
    "    torch.save(model.state_dict(), f'model_fold_50_10{fold}.pth')\n",
    "    np.savetxt(f'threshold_fold_50_10{fold}.txt', [threshold])\n",
    "\n",
    "average_threshold = np.mean(thresholds)\n",
    "print(f'Average Anomaly Detection Threshold: {average_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#10\n",
    "Average Anomaly Detection Threshold: 0.01782991538445155'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d28494",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8331c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Scores per Fold: [0.755075227982145, 0.7431207950795419, 0.7485879502587613]\n",
      "Mean AUC-ROC: 0.7489, Standard Deviation: 0.0049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'scaler' is the MinMaxScaler fitted on your original dataset\n",
    "\n",
    "# Load and preprocess the new dataset\n",
    "new_df = pd.read_csv('df_testing.csv')\n",
    "\n",
    "# It seems you're intending to use a binary classification ('normal' vs. not 'normal')\n",
    "# Assuming 'class3' is your label column\n",
    "X_new = new_df.drop('class3', axis=1).values\n",
    "y_new = new_df['class3'].values\n",
    "\n",
    "# Normalize the new dataset\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Convert to tensors\n",
    "X_new_tensor = torch.tensor(X_new_scaled, dtype=torch.float32)\n",
    "y_new_tensor = torch.tensor(y_new, dtype=torch.long)\n",
    "\n",
    "# Define a function to evaluate a model\n",
    "def evaluate_model(model, X_tensor, y_tensor):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=32)\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            reconstruction, _, _ = model(data)\n",
    "            recon_error = torch.mean((data - reconstruction) ** 2, dim=1)\n",
    "            predictions.extend(recon_error.numpy())\n",
    "\n",
    "    # Assuming anomalies are labeled as '1' in 'y_new'\n",
    "    auc_roc = roc_auc_score(y_new, predictions)\n",
    "    return auc_roc\n",
    "\n",
    "# Load models and calculate AUC-ROC for each\n",
    "auc_roc_scores = []\n",
    "n_folds = 3\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    model = FusionVAE(input_dim=78, latent_dim=20)\n",
    "    model.load_state_dict(torch.load(f'model_fold_{fold}.pth'))\n",
    "    \n",
    "    auc_roc = evaluate_model(model, X_new_tensor, y_new_tensor)\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "\n",
    "# Calculate mean and standard deviation of AUC-ROC scores\n",
    "mean_auc_roc = np.mean(auc_roc_scores)\n",
    "std_auc_roc = np.std(auc_roc_scores)\n",
    "\n",
    "print(f\"AUC-ROC Scores per Fold: {auc_roc_scores}\")\n",
    "print(f\"Mean AUC-ROC: {mean_auc_roc:.4f}, Standard Deviation: {std_auc_roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42be55f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#10\\nAUC-ROC Scores per Fold: [0.755075227982145, 0.7431207950795419, 0.7485879502587613]\\nMean AUC-ROC: 0.7489, Standard Deviation: 0.0049'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#10\n",
    "AUC-ROC Scores per Fold: [0.755075227982145, 0.7431207950795419, 0.7485879502587613]\n",
    "Mean AUC-ROC: 0.7489, Standard Deviation: 0.0049'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988a9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per Fold: [0.623597105907822, 0.6286859962985811, 0.6489564774751845]\n",
      "Mean Precision: 0.6337, Standard Deviation: 0.0110\n",
      "\n",
      "Recall per Fold: [0.13702368434229634, 0.1275724800961394, 0.1276726253066947]\n",
      "Mean Recall: 0.1308, Standard Deviation: 0.0044\n",
      "\n",
      "F1_score per Fold: [0.2246785102168581, 0.21210481403625614, 0.21336820083682007]\n",
      "Mean F1_score: 0.2167, Standard Deviation: 0.0057\n",
      "\n",
      "Accuracy per Fold: [0.5398311485795039, 0.5388138979581891, 0.5419204717119048]\n",
      "Mean Accuracy: 0.5402, Standard Deviation: 0.0013\n",
      "\n",
      "Auc_roc per Fold: [0.7557301141089585, 0.7424160114566101, 0.7480718422995676]\n",
      "Mean Auc_roc: 0.7487, Standard Deviation: 0.0055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming the rest of your setup is unchanged...\n",
    "\n",
    "# Update the evaluate_model function to calculate additional metrics\n",
    "def evaluate_model(model, X_tensor, y_true, threshold):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(TensorDataset(X_tensor, y_true), batch_size=32)\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            reconstruction, _, _ = model(data)\n",
    "            recon_error = torch.mean((data - reconstruction) ** 2, dim=1)\n",
    "            predictions.extend(recon_error.numpy())\n",
    "    \n",
    "    # Convert reconstruction errors to binary predictions based on the threshold\n",
    "    binary_predictions = np.array(predictions) > threshold\n",
    "    \n",
    "    # Calculate metrics\n",
    "    p = precision_score(y_true.numpy(), binary_predictions)\n",
    "    r = recall_score(y_true.numpy(), binary_predictions)\n",
    "    f1 = f1_score(y_true.numpy(), binary_predictions)\n",
    "    cm = confusion_matrix(y_true.numpy(), binary_predictions)\n",
    "    acc = accuracy_score(y_true.numpy(), binary_predictions)\n",
    "    auc_roc = roc_auc_score(y_true.numpy(), predictions)\n",
    "\n",
    "    return {'precision': p, 'recall': r, 'f1_score': f1, 'confusion_matrix': cm, 'accuracy': acc, 'auc_roc': auc_roc}\n",
    "\n",
    "# Load models, calculate metrics for each, and aggregate results\n",
    "metrics = {'precision': [], 'recall': [], 'f1_score': [], 'accuracy': [], 'auc_roc': []}\n",
    "n_folds = 3\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    # Load the saved model and threshold\n",
    "    model = FusionVAE(input_dim=78, latent_dim=20)\n",
    "    model.load_state_dict(torch.load(f'model_fold_{fold}.pth'))\n",
    "    threshold = np.loadtxt(f'threshold_fold_{fold}.txt')\n",
    "    \n",
    "    fold_metrics = evaluate_model(model, X_new_tensor, y_new_tensor, threshold)\n",
    "    \n",
    "    for key in metrics:\n",
    "        metrics[key].append(fold_metrics[key])\n",
    "\n",
    "# Calculate mean and standard deviation of metrics\n",
    "for metric in metrics:\n",
    "    mean_value = np.mean(metrics[metric])\n",
    "    std_value = np.std(metrics[metric])\n",
    "    print(f\"{metric.capitalize()} per Fold: {metrics[metric]}\")\n",
    "    print(f\"Mean {metric.capitalize()}: {mean_value:.4f}, Standard Deviation: {std_value:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8924d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new dataset\n",
    "new_df = pd.read_csv('df_testing.csv')\n",
    "#new_df['42 Labels'] = (new_df['42 Labels'] != 'normal').astype('int64')\n",
    "#new_df['42 Labels'] = (new_df['42 Labels'] != 'normal').astype('int64')\n",
    "#new_df = new_df[new_df['42 Labels'] == 0].drop('42 Labels', axis=1)\n",
    "\n",
    "# Normalize the new dataset\n",
    "X_new = new_df.drop('class3', axis=1)\n",
    "y_new = new_df['class3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "182104f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164168, 79)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76c750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\XAAE-IIoT\\ON\n"
     ]
    }
   ],
   "source": [
    "cd C:\\adversarial_analysis-master\\Investigating_the_Practicality_of_Adversarial_Evasion_Attacks_on_Network_Intrusion_Detection\\XAAE-IIoT\\ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8073dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            reconstruction, _, _ = model(data)\n",
    "            recon_error = torch.mean((data - reconstruction) ** 2, dim=1)\n",
    "            # Ensure threshold is a tensor or a scalar\n",
    "            pred = (recon_error > torch.tensor(threshold, dtype=torch.float32)).int()\n",
    "            predictions.extend(pred.numpy())\n",
    "            actuals.extend(labels.numpy())\n",
    "    return predictions, actuals\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "\n",
    "# Specify the files to process\n",
    "files_to_process = [\n",
    "    'FSGM-XAAE-IIoT.csv', 'BIM-XAAE-IIoT.csv', 'DF-XAAE-IIoT.csv',\n",
    "    'JSMA-XAAE-IIoT.csv', 'CW2-XAAE-IIoT.csv', 'CWinf-XAAE-IIoT.csv'\n",
    "]\n",
    "\n",
    "ref_file = 'df_testing.csv'\n",
    "ref_df = pd.read_csv(ref_file)\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "for fold in range(3):  # Assuming 3 folds\n",
    "    model = FusionVAE(input_dim=78, latent_dim=20)  # Adjust parameters as necessary\n",
    "    model_path = f'model_fold_{fold}.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    threshold = np.loadtxt(f'threshold_fold_50_10{fold}.txt')\n",
    "    \n",
    "    for file_name in files_to_process:\n",
    "        df = pd.read_csv(file_name)\n",
    "        df.columns = ref_df.columns\n",
    "        concatenated_df = pd.concat([ref_df, df], ignore_index=True)\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(concatenated_df.iloc[:, :-1])\n",
    "        y_true = concatenated_df.iloc[:, -1].values\n",
    "        \n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_true, dtype=torch.long)\n",
    "        \n",
    "        data_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=32, shuffle=False)\n",
    "        predictions, actuals = evaluate_model(model, data_loader, threshold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(actuals, predictions)\n",
    "        precision = precision_score(actuals, predictions)\n",
    "        recall = recall_score(actuals, predictions)\n",
    "        f1 = f1_score(actuals, predictions)\n",
    "        roc_auc = roc_auc_score(actuals, predictions)\n",
    "        cm = confusion_matrix(actuals, predictions).flatten().tolist()  # Flatten CM to save in CSV\n",
    "        \n",
    "        results.append([fold, file_name, accuracy, precision, recall, f1, roc_auc] + cm)\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "columns = ['Fold', 'File', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC_ROC', \n",
    "           'TN', 'FP', 'FN', 'TP']  # Adjust as needed\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "results_df.to_csv('evaluation_metrics50_10.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "778f4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Assuming your model class and evaluate_model function are defined correctly...\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "\n",
    "files_to_process = [\n",
    "    'FSGM-XAAE-IIoT.csv', 'BIM-XAAE-IIoT.csv', 'DF-XAAE-IIoT.csv',\n",
    "    'JSMA-XAAE-IIoT.csv', 'CW2-XAAE-IIoT.csv', 'CWinf-XAAE-IIoT.csv'\n",
    "]\n",
    "\n",
    "ref_file = 'df_testing.csv'\n",
    "ref_df = pd.read_csv(ref_file)\n",
    "\n",
    "# Structure to hold raw results\n",
    "raw_results = {file: [] for file in files_to_process}\n",
    "\n",
    "for fold in range(3):  # For each fold\n",
    "    model_path = f'model_fold_{fold}.pth'\n",
    "    threshold = np.loadtxt(f'threshold_fold_{fold}.txt')\n",
    "\n",
    "    # Load model\n",
    "    model = FusionVAE(input_dim=78, latent_dim=20)  # Initialize your model here\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    for file_name in files_to_process:\n",
    "        df = pd.read_csv(file_name)\n",
    "        df.columns = ref_df.columns\n",
    "        concatenated_df = pd.concat([ref_df, df], ignore_index=True)\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(concatenated_df.iloc[:, :-1])\n",
    "        y_true = concatenated_df.iloc[:, -1].values\n",
    "        \n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_true, dtype=torch.long)\n",
    "        \n",
    "        data_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=32, shuffle=False)\n",
    "        \n",
    "        predictions, actuals = evaluate_model(model, data_loader, threshold)\n",
    "        \n",
    "        # Compute metrics\n",
    "        roc_auc = roc_auc_score(actuals, predictions)\n",
    "        \n",
    "        raw_results[file_name].append(roc_auc)\n",
    "\n",
    "# Prepare final results with mean/std AUC-ROC\n",
    "final_results = []\n",
    "\n",
    "for file_name, auc_roc_scores in raw_results.items():\n",
    "    mean_auc_roc = np.mean(auc_roc_scores)\n",
    "    std_auc_roc = np.std(auc_roc_scores)\n",
    "    final_results.append([file_name, mean_auc_roc, std_auc_roc])\n",
    "\n",
    "# Convert final results to a DataFrame and save to CSV\n",
    "final_df = pd.DataFrame(final_results, columns=['File', 'AUC_ROC_Mean', 'AUC_ROC_STD'])\n",
    "final_df.sort_values('File', inplace=True)\n",
    "final_df.to_csv('final_evaluation_metrics50_10.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d5d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
